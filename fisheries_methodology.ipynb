{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FISHERIES MODELLING\n",
    "\n",
    "The main goal in this code is to create the methodology to model fish stock change with different climate change scenarios combining the Exploitable Biommas Index that can be obtained from the SPiCT model, envrionmental variables and a machine learning algorithm (potentially GAMs). As the SPiCT model results should be used in relative terms, the fish stock changes in Climate Change scenarios should also be reported in relative terms.\n",
    "\n",
    "The steps that we have to follow are the following ones:\n",
    "1. Prepare environmental data.\n",
    "2. Predictor selection\n",
    "3. Model training, testing and validation\n",
    "4. Run projections\n",
    "\n",
    "Python version: 3.1.2\n",
    "\n",
    "All results will be saved in a new folder called 'Fisheries_modelling' saved in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENTAL DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries to prepare the environmental data:\n",
    "import os \n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import mapping\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input code-block:\n",
    "\n",
    "    # Xarray is not able to identify the EPSG code or coordinate system of the Copernicus netCDF, so the user should manually check the EPSG code in a GIS software and introduce it:\n",
    "\n",
    "        # Environmental variables EPSG code:\n",
    "env_epsg = f\"EPSG:{input('Environmental variables EPSG code:')}\"\n",
    "\n",
    "    # Working directory where environmental variable netCDFs are:\n",
    "wd = r\"C:\\PhD\\Copernicus_env_data_fisheries\\Cadiz\"\n",
    "\n",
    "    # First prediction year (year after last landings data):\n",
    "start_year = float(input('First prediction year:'))\n",
    "\n",
    "    # First envionmental data time:\n",
    "env_data_start = float(input('First environmental data year:')) # NOTICE that if the first data is not in 1rst January, as the input has to be floating type, you should use 'input = year + ((month_first_data - 1)/12)' to compute input.\n",
    "\n",
    "    # Name of the shapefile that delimits the region of interest:\n",
    "roi_file = f\"{input('Name of the shapefile that delimits the region of interest:')}\"\n",
    "        # Ensure the shapefile name is correct:\n",
    "if '.shp' in roi_file:\n",
    "    pass\n",
    "else:\n",
    "    roi_file = roi_file + '.shp'\n",
    "\n",
    "    # Name of the csv where the relative biomass estimates obtained from the SPiCT model are stored:\n",
    "spict_data = f\"{input(\"Name of the csv where the SPiCT estimates and date of estimates are:\")}\"\n",
    "        # Ensure the csv name is correct:\n",
    "if '.csv' in spict_data:\n",
    "    pass\n",
    "else: \n",
    "    spict_data = spict_data + '.csv'\n",
    "\n",
    "    # Biomass at Maximum Sustainable Yield obtained from SPiCT:\n",
    "bmsy = float(input('Biomass at Maximum Sustainable Yield computed from SPiCT model:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory:\n",
    "os.chdir(wd)\n",
    "# Define the file names and store the netCDFs in an array:\n",
    "netcdfs = [nc for nc in os.listdir(wd) if \".nc\" in nc[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file where we have the SPiCT relative biomass estimates as well as the date of each estimation:\n",
    "dataset = pd.read_csv(spict_data)\n",
    "\n",
    "# Clean the SPiCT dataset to keep just estimates of the dates where environmental data are available (also delete predictions of the SPiCT):\n",
    "dataset = dataset.drop(dataset[(dataset['time'] > start_year) | (dataset['time'] < env_data_start)].index)\n",
    "\n",
    "# Function to convert decimal years of the csv to date type data:\n",
    "def decimal_year_to_ym(decimal_year):\n",
    "    year = np.floor(decimal_year).astype(int) \n",
    "    remainder = decimal_year - year  \n",
    "    month = np.floor(remainder * 12).astype(int) + 1 \n",
    "    return pd.to_datetime(year.astype(str) + '-' + month.astype(str).str.zfill(2), format='%Y-%m')\n",
    "\n",
    "# Execute function to change 'time' column to a date type:\n",
    "dataset['time'] = decimal_year_to_ym(dataset['time'])\n",
    "\n",
    "# If two estimates are for one month compute the average to have just 1 relative biomass estimate by month:\n",
    "dataset_cleared = dataset.groupby('time', as_index=False)['relative_B'].mean()  # Notice that 'time' and 'relative_B' are the field names in the original SPiCT estimate csv\n",
    "\n",
    "# Accumulator:\n",
    "i_bio = 0\n",
    "i_phy = 0\n",
    "\n",
    "# Loop the dataset and make the needed computations to prepare the input dataset:\n",
    "\n",
    "for env_netcdf in netcdfs:\n",
    "    # 1. Open the dataset and define the coordinate system:\n",
    "    try:\n",
    "        ds = xr.open_dataset(env_netcdf, engine=\"h5netcdf\")\n",
    "        ds = ds.rio.write_crs(env_epsg, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file with xarray: {e}\")\n",
    "\n",
    "    # 2. Mask the dataset to keep just the depth ranges that will be used:\n",
    "    depth_mask_up = ((ds['depth'] > 5) & (ds['depth'] < 10))\n",
    "    depth_mask_low = ((ds['depth'] > 45) & (ds['depth'] < 100))\n",
    "\n",
    "    # 3. Filter the datasets to keep just the desired depths:\n",
    "    ds_up = ds.where(depth_mask_up, drop=True)\n",
    "    ds_low = ds.where(depth_mask_low, drop=True)\n",
    "\n",
    "    # 4. Compute average values in the dimension 'depth':\n",
    "    mean_up = ds_up.mean(dim='depth')\n",
    "    mean_low = ds_low.mean(dim='depth')\n",
    "\n",
    "    # 5. Rename the variable names to be able to differentiate them:\n",
    "    if 'Biogeochemical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'chl': 'chl_up', 'nppv': 'npp_up','o2': 'o2_up','no3': 'no3_up','po4': 'po4_up','nh4': 'nh4_up','ph': 'ph_up'})\n",
    "        mean_low = mean_low.rename({'chl': 'chl_low', 'nppv': 'npp_low','o2': 'o2_low','no3': 'no3_low','po4': 'po4_low','nh4': 'nh4_low','ph': 'ph_low'})\n",
    "        \n",
    "    elif 'Physical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'so': 'so_up','thetao': 'thetao_up'})\n",
    "        mean_low = mean_low.rename({'so':'so_low', 'thetao': 'thetao_low'})\n",
    "        \n",
    "    else: \n",
    "        print(f\"{env_netcdf} netCDF file names are not included in the loop. Check the code to adapt it!\")\n",
    "        exit()\n",
    "\n",
    "    # 6. Read the region of interest shapefile and check if the coordinate system of the environmental variables and the shapefile is the same:\n",
    "    roi = gpd.read_file(roi_file)\n",
    "    find_env_epsg = re.search(r'AUTHORITY\\[\"EPSG\",\"(\\d+)\"\\]', str(mean_low.rio.crs)) # Check coordinate system of env. data\n",
    "    if find_env_epsg.group(1) == roi.crs.to_epsg():\n",
    "        print(\"The EPSG codes of the environmental variables and the region of interest are the same!\")\n",
    "    else: \n",
    "        print(f\"The EPSG codes of the inputs are different! Environmental data projection is EPSG:{find_env_epsg.group(1)} and Region of Interest projection is EPSG:{roi.crs.to_epsg()}\")\n",
    "        print(f\"Converting Region of Interest file projection to EPSG:{find_env_epsg.group(1)}\")\n",
    "        roi = roi.to_crs(epsg=find_env_epsg.group(1))\n",
    "\n",
    "    # 7. Extract the geometry of the region of interest and clip the environmental data to the region of interest:\n",
    "    roi_mask = [mapping(roi.geometry[0])]   # Extract roi geometry\n",
    "    mean_up_clip = mean_up.rio.clip(roi_mask, roi.crs, drop=True)\n",
    "    mean_low_clip = mean_low.rio.clip(roi_mask, roi.crs, drop=True)\n",
    "\n",
    "    # 8. Compute the mean in each depth range and within the region of interest:\n",
    "    avg_up = mean_up_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    avg_low = mean_low_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # 9. Convert netCDF into a dataframe:\n",
    "    env_dataframe_up = avg_up.to_dataframe().reset_index()\n",
    "    env_dataframe_low = avg_low.to_dataframe().reset_index()\n",
    "\n",
    "    # 10. Filter the dataframes to drop the 'spatial_ref' column that can cause problems:\n",
    "    if 'spatial_ref' in env_dataframe_up.columns:\n",
    "        env_dataframe_up = env_dataframe_up.drop(columns=['spatial_ref'])\n",
    "    if 'spatial_ref' in env_dataframe_low.columns:\n",
    "        env_dataframe_low = env_dataframe_low.drop(columns=['spatial_ref'])\n",
    "\n",
    "    # 11. Merge the dataframes aligning the biomass estimates and environmental variable dates:\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_up, on='time', how='left')\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_low, on='time', how='left')\n",
    "\n",
    "# 12. Create a list of the duplicate columns to merge:\n",
    "cols_to_combine = ['chl_up', 'npp_up', 'o2_up', 'no3_up', 'po4_up', 'nh4_up', 'ph_up',\n",
    "                   'chl_low', 'npp_low', 'o2_low', 'no3_low', 'po4_low', 'nh4_low', 'ph_low',\n",
    "                   'so_up', 'thetao_up', 'so_low', 'thetao_low']\n",
    "\n",
    "# 13. For each column, merge the versions \"_x\" & \"_y\" using combine_first():\n",
    "for col in cols_to_combine:\n",
    "    dataset_cleared[col] = dataset_cleared[f'{col}_x'].combine_first(dataset_cleared[f'{col}_y'])\n",
    "\n",
    "# 14. Delete the duplicate columns (_x, _y) as the data was already merged:\n",
    "dataset_cleared = dataset_cleared.drop(columns=[f'{col}_x' for col in cols_to_combine] + [f'{col}_y' for col in cols_to_combine])\n",
    "\n",
    "# 15. Create a new column with the values of catch/Bmsy (harmonized by month) that matches 'time'\n",
    "    # Read SPiCT input csv again to avoid previous modifications:\n",
    "captures_data = pd.read_csv(spict_data)\n",
    "    # Ensure 'timeC' is a floating type data:\n",
    "captures_data['timeC'] = captures_data['timeC'].astype(float)\n",
    "    # Drop NoData:\n",
    "captures_data = captures_data.dropna(subset=['timeC'])\n",
    "    # Create a Dataframe to store monthly means in each quarter:\n",
    "monthly_captures = []\n",
    "\n",
    "    # Loop in each quarter and compute the mean:\n",
    "for index, row in captures_data.iterrows():\n",
    "    year = int(row['timeC'])  \n",
    "    quarter = int((row['timeC'] - year) * 100 / 25) + 1  \n",
    "    \n",
    "    # Calcular los meses correspondientes\n",
    "    if quarter == 1:\n",
    "        months = [f\"{year}-01\", f\"{year}-02\", f\"{year}-03\"]\n",
    "    elif quarter == 2:\n",
    "        months = [f\"{year}-04\", f\"{year}-05\", f\"{year}-06\"]\n",
    "    elif quarter == 3:\n",
    "        months = [f\"{year}-07\", f\"{year}-08\", f\"{year}-09\"]\n",
    "    elif quarter == 4:\n",
    "        months = [f\"{year}-10\", f\"{year}-11\", f\"{year}-12\"]\n",
    "    \n",
    "    # Asign the mean of the captures to the corresponding months:\n",
    "    monthly_captures.extend([(month, row['obsC'] / 3) for month in months])\n",
    "\n",
    "    # Convert the list to a Dataframe:\n",
    "monthly_captures_df = pd.DataFrame(monthly_captures, columns=['time', 'obsC'])\n",
    "\n",
    "    # Convert to 'time' datatype:\n",
    "monthly_captures_df['time'] = pd.to_datetime(monthly_captures_df['time'])\n",
    "\n",
    "    # Compute catch/Bmsy:\n",
    "monthly_captures_df['rel_C'] = monthly_captures_df['obsC']/bmsy\n",
    "\n",
    "    # Merge 'dataset_cleared' with 'monthly_captures_df' to add captures into the training dataset:\n",
    "dataset_cleared = pd.merge(dataset_cleared, monthly_captures_df, on='time', how='left')\n",
    "\n",
    "# 16. Drop any timestep with Nan:\n",
    "dataset_cleared = dataset_cleared.dropna(how='any')\n",
    "\n",
    "# 17. Save the dataset that will be used to train the model:\n",
    "os.makedirs('Fisheries_modelling')\n",
    "dataset_cleared.to_csv(os.path.join('Fisheries_modelling', 'training_dataset.csv'))\n",
    "\n",
    "print(\"TRAINING DATASET PREPARED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTOR SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunks we will execute the necessary methodologies to perform the selection of predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the csv with training data:\n",
    "training_data = pd.read_csv(os.path.join('Fisheries_modelling', 'training_dataset.csv'))\n",
    "\n",
    "# 2. Clear the csv if 'Unnamed' columns are in the training dataset:\n",
    "training_data = training_data.loc[:, ~training_data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# 3. Convert 'time' column into datetime type:\n",
    "training_data['time'] = pd.to_datetime(training_data['time'])\n",
    "\n",
    "# 4. Create a list with the first year of data and steps of 5 years (they will be used in the plots):\n",
    "start_year = training_data['time'].dt.year.min()\n",
    "end_year = training_data['time'].dt.year.max()\n",
    "years = np.arange(start_year, end_year + 1, 5)\n",
    "\n",
    "# 5. Predictor list:\n",
    "predictors_names = cols_to_combine + ['previous_relative_B']  # Adjust your predictors\n",
    "\n",
    "# 6. Create a loop to plot relative Biomass Index-Predictor Variable pairs:\n",
    "for predictor in predictors_names:\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # First axis for relative biomass:\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Relative Biomass', color='blue')\n",
    "    ax1.plot(training_data['time'], training_data['relative_B'], label='Relative Biomass', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')  # Axis colour\n",
    "\n",
    "    # Configure the ticks of the axis\n",
    "    ax1.set_xticks([pd.Timestamp(f'{year}-01-01') for year in years])  # Use the list of years\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Just show the year\n",
    "    \n",
    "    # Establish the limits for the x axis ticks:\n",
    "    ax1.set_xlim([training_data['time'].min() - pd.DateOffset(years=1), training_data['time'].max()])\n",
    "\n",
    "    \n",
    "    # Rotate the xaxis ticks\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Second axis for the predictor\n",
    "    ax2 = ax1.twinx()  # Create a second y axis that shares x axis\n",
    "    ax2.set_ylabel(predictor, color='green')\n",
    "    ax2.plot(training_data['time'], training_data[predictor], label=predictor, color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')  # y second axis colour\n",
    "    \n",
    "    # Plot title\n",
    "    plt.title(f'Relative Biomass and {predictor} over Time')\n",
    "\n",
    "    # Ensure axis disposition is correct\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Show plot:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisheries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
