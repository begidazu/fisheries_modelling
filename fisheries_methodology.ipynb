{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FISHERIES MODELLING\n",
    "\n",
    "The main goal in this code is to create the methodology to model fish stock change with different climate change scenarios combining the Exploitable Biommas Index that can be obtained from the SPiCT model, envrionmental variables and a machine learning algorithm (potentially GAMs). As the SPiCT model results should be used in relative terms, the fish stock changes in Climate Change scenarios should also be reported in relative terms.\n",
    "\n",
    "The steps that we have to follow are the following ones:\n",
    "1. Prepare environmental data.\n",
    "2. \n",
    "3. \n",
    "\n",
    "Python version: 3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENTAL DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries to prepare the environmental data:\n",
    "import os \n",
    "import xarray as xr\n",
    "from  netCDF4 import Dataset\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "import rasterio\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input code-block:\n",
    "\n",
    "    # Xarray is not able to identify the EPSG code or coordinate system of the Copernicus netCDF, so the user should manually check the EPSG code in a GIS software and introduce it:\n",
    "\n",
    "        # Environmental variables EPSG code:\n",
    "env_epsg = f\"EPSG:{input('Environmental variables EPSG code:')}\"\n",
    "\n",
    "    # Working directory where environmental variable netCDFs are:\n",
    "wd = r\"C:\\PhD\\Copernicus_env_data_fisheries\\Cadiz\"\n",
    "\n",
    "    # First prediction year (year after last landings data):\n",
    "start_year = float(input('First prediction year:'))\n",
    "\n",
    "    # First envionmental data time:\n",
    "env_data_start = float(input('First environmental data year:')) # NOTICE that if the first data is not in 1rst January, as the input has to be floating type, you should use 'input = year + ((month_first_data - 1)/12)' to compute input.\n",
    "\n",
    "    # Name of the shapefile that delimits the region of interest:\n",
    "roi_file = f\"{input('Name of the shapefile that delimits the region of interest:')}\"\n",
    "\n",
    "    # Name of the csv where the relative biomass estimates obtained from the SPiCT model are stored:\n",
    "spict_data = f\"{input(\"Name of the csv where the SPiCT estimates and date of estimates are:\")}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory:\n",
    "os.chdir(wd)\n",
    "\n",
    "# Define the file names and store the netCDFs in an array:\n",
    "netcdfs = [nc for nc in os.listdir(wd) if \".nc\" in nc[-3:]]\n",
    "print(netcdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file where we have the SPiCT relative biomass estimates as well as the date of each estimation:\n",
    "dataset = pd.read_csv(spict_data)\n",
    "\n",
    "# Clean the SPiCT dataset to keep just estimates of the dates where environmental data are available (also delete predictions of the SPiCT):\n",
    "dataset = dataset.drop(dataset[(dataset['time'] > start_year) | (dataset['time'] < env_data_start)].index)\n",
    "\n",
    "# Function to convert decimal years of the csv to date type data:\n",
    "def decimal_year_to_ym(decimal_year):\n",
    "    year = np.floor(decimal_year).astype(int) \n",
    "    remainder = decimal_year - year  \n",
    "    month = np.floor(remainder * 12).astype(int) + 1 \n",
    "    return pd.to_datetime(year.astype(str) + '-' + month.astype(str).str.zfill(2), format='%Y-%m')\n",
    "\n",
    "# Execute function to change 'time' column to a date type:\n",
    "dataset['time'] = decimal_year_to_ym(dataset['time'])\n",
    "\n",
    "# If two estimates are for one month compute the average to have just 1 relative biomass estimate by month:\n",
    "dataset_cleared = dataset.groupby('time', as_index=False)['relative_B'].mean()  # Notice that 'time' and 'relative_B' are the field names in the original SPiCT estimate csv\n",
    "\n",
    "# Accumulator:\n",
    "i_bio = 0\n",
    "i_phy = 0\n",
    "\n",
    "# Loop the dataset and make the needed computations to prepare the input dataset:\n",
    "\n",
    "for env_netcdf in netcdfs:\n",
    "    # 1. Open the dataset and define the coordinate system:\n",
    "    try:\n",
    "        ds = xr.open_dataset(env_netcdf, engine=\"h5netcdf\")\n",
    "        ds = ds.rio.write_crs(env_epsg, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file with xarray: {e}\")\n",
    "\n",
    "    # 2. Mask the dataset to keep just the depth ranges that will be used:\n",
    "    depth_mask_up = ((ds['depth'] > 5) & (ds['depth'] < 10))\n",
    "    depth_mask_low = ((ds['depth'] > 45) & (ds['depth'] < 100))\n",
    "\n",
    "    # 3. Filter the datasets to keep just the desired depths:\n",
    "    ds_up = ds.where(depth_mask_up, drop=True)\n",
    "    ds_low = ds.where(depth_mask_low, drop=True)\n",
    "\n",
    "    # 4. Compute average values in the dimension 'depth':\n",
    "    mean_up = ds_up.mean(dim='depth')\n",
    "    mean_low = ds_low.mean(dim='depth')\n",
    "    print(list(mean_up.data_vars.keys()))\n",
    "\n",
    "    # 5. Rename the variable names to be able to differentiate them:\n",
    "    if 'Biogeochemical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'chl': 'chl_up', 'nppv': 'npp_up','o2': 'o2_up','no3': 'no3_up','po4': 'po4_up','nh4': 'nh4_up','ph': 'ph_up'})\n",
    "        mean_low = mean_low.rename({'chl': 'chl_low', 'nppv': 'npp_low','o2': 'o2_low','no3': 'no3_low','po4': 'po4_low','nh4': 'nh4_low','ph': 'ph_low'})\n",
    "        \n",
    "    elif 'Physical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'so': 'so_up','thetao': 'thetao_up'})\n",
    "        mean_low = mean_low.rename({'so':'so_low', 'thetao': 'thetao_low'})\n",
    "        \n",
    "    else: \n",
    "        print(\"netCDF file names are not included in the loop. Check the code to adapt it!\")\n",
    "        exit()\n",
    "\n",
    "    # 6. Read the region of interest shapefile and check if the coordinate system of the environmental variables and the shapefile is the same:\n",
    "    roi = gpd.read_file(roi_file)\n",
    "    find_env_epsg = re.search(r'AUTHORITY\\[\"EPSG\",\"(\\d+)\"\\]', str(mean_low.rio.crs)) # Check coordinate system of env. data\n",
    "    if find_env_epsg.group(1) == roi.crs.to_epsg():\n",
    "        print(\"The EPSG codes of the environmental variables and the region of interest are the same!\")\n",
    "    else: \n",
    "        print(f\"The EPSG codes of the inputs are different! Environmental data projection is EPSG:{find_env_epsg.group(1)} and Resion of Interest projection is EPSG:{roi.crs.to_epsg()}\")\n",
    "        print(f\"Converting Region of Interest file projection to EPSG:{find_env_epsg.group(1)}\")\n",
    "        roi_reprojected = roi.to_crs(epsg=find_env_epsg.group(1))\n",
    "\n",
    "    # 7. Extract the geometry of the region of interest and clip the environmental data to the region of interest:\n",
    "    roi_mask = [mapping(roi_reprojected.geometry[0])]   # Extract roi geometry\n",
    "    mean_up_clip = mean_up.rio.clip(roi_mask, roi_reprojected.crs, drop=True)\n",
    "    mean_low_clip = mean_low.rio.clip(roi_mask, roi_reprojected.crs, drop=True)\n",
    "    \n",
    "    # 8. Compute the mean in each depth range and within the region of interest:\n",
    "    avg_up = mean_up_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    avg_low = mean_low_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # 9. Convert netCDF into a dataframe:\n",
    "    env_dataframe_up = avg_up.to_dataframe().reset_index()\n",
    "    env_dataframe_low = avg_low.to_dataframe().reset_index()\n",
    "\n",
    "    # 10. Filter the dataframes to drop the 'spatial_ref' column that can cause problems:\n",
    "    if 'spatial_ref' in env_dataframe_up.columns:\n",
    "        env_dataframe_up = env_dataframe_up.drop(columns=['spatial_ref'])\n",
    "    if 'spatial_ref' in env_dataframe_low.columns:\n",
    "        env_dataframe_low = env_dataframe_low.drop(columns=['spatial_ref'])\n",
    "\n",
    "    # 11. Merge the dataframes aligning the biomass estimates and environmental variable dates:\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_up, on='time', how='left')\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_low, on='time', how='left')\n",
    "\n",
    "# 12. Create a list of the duplicate columns to merge:\n",
    "cols_to_combine = ['chl_up', 'npp_up', 'o2_up', 'no3_up', 'po4_up', 'nh4_up', 'ph_up',\n",
    "                   'chl_low', 'npp_low', 'o2_low', 'no3_low', 'po4_low', 'nh4_low', 'ph_low',\n",
    "                   'so_up', 'thetao_up', 'so_low', 'thetao_low']\n",
    "\n",
    "# 13. For each column, merge the versions \"_x\" & \"_y\" using combine_first():\n",
    "for col in cols_to_combine:\n",
    "    dataset_cleared[col] = dataset_cleared[f'{col}_x'].combine_first(dataset_cleared[f'{col}_y'])\n",
    "\n",
    "# 14. Delete the duplicate columns (_x, _y) as the data was already merged:\n",
    "dataset_cleared = dataset_cleared.drop(columns=[f'{col}_x' for col in cols_to_combine] + [f'{col}_y' for col in cols_to_combine])\n",
    "\n",
    "# 15. Create a new column with the predictor variable 'previous_relative_B' as the relative Biomass at time x could potentially be influenced by the relative Biomass at time x-1:\n",
    "dataset_cleared['previous_relative_B'] = dataset_cleared['relative_B'].shift(1)\n",
    "\n",
    "# 16. Drop any timestep with Nan:\n",
    "dataset_cleared = dataset_cleared.dropna(how='any')\n",
    "\n",
    "# 17. Save the dataset that will be used to train the model:\n",
    "dataset_cleared.to_csv('training_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisheries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
