{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FISHERIES MODELLING\n",
    "\n",
    "The main goal in this code is to create the methodology to model fish stock change with different climate change scenarios combining the Exploitable Biommas Index that can be obtained from the SPiCT model, envrionmental variables, landing/catch data and a machine learning algorithm (potentially GAMs). As the SPiCT model results should be used in relative terms, the fish stock changes in Climate Change scenarios should also be reported in relative terms.\n",
    "\n",
    "The steps that we have to follow are the following ones:\n",
    "1. Prepare environmental data.\n",
    "2. Predictor selection\n",
    "3. Model training, testing and validation\n",
    "4. Run projections\n",
    "\n",
    "Python version: 3.1.2\n",
    "\n",
    "All results will be saved in a new folder called 'Fisheries_modelling' saved in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import mapping\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER INPUTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory where environmental variable netCDFs are:\n",
    "wd = r\"C:\\PhD\\Copernicus_env_data_fisheries\\Cadiz\"\n",
    "\n",
    "# Environmental variables EPSG code:\n",
    "env_epsg = f\"EPSG:{input('Environmental variables EPSG code:')}\" # Xarray is not able to identify the EPSG code or coordinate system of the Copernicus netCDF, so the user should manually check the EPSG code in a GIS software and introduce it:\n",
    "\n",
    "# Name of the shapefile that delimits the region of interest:\n",
    "roi_file = f\"{input('Name of the shapefile that delimits the region of interest:')}\"\n",
    "    # Ensure the shapefile name is correct:\n",
    "if '.shp' in roi_file:\n",
    "    pass\n",
    "else:\n",
    "    roi_file = roi_file + '.shp'\n",
    "\n",
    "# Name of the csv where the relative biomass estimates obtained from the SPiCT model are stored:\n",
    "spict_data = f\"{input(\"Name of the csv where the SPiCT estimates and date of estimates are:\")}\"\n",
    "    # Ensure the csv name is correct:\n",
    "if '.csv' in spict_data:\n",
    "    pass\n",
    "else: \n",
    "    spict_data = spict_data + '.csv'\n",
    "\n",
    "# Biomass at Maximum Sustainable Yield obtained from SPiCT:\n",
    "bmsy = float(input('Biomass at Maximum Sustainable Yield computed from SPiCT model:'))\n",
    "\n",
    "# Landing and spawning predictor inputs:\n",
    "\n",
    "    # Maximum age of stock:\n",
    "age_max = int(input('What is the maximum age of the stock?'))\n",
    "\n",
    "    # Number of spawnings during a year:\n",
    "n_spawning = int(input('How many spawnings (number) are during a year?'))\n",
    "\n",
    "if n_spawning == 1:\n",
    "    # Season of spawning:\n",
    "    first_spawning = f\"{input('Define when the first spawning occurs (e.g. May). If it occurs in more than one month write April,May,June for example.')}\"\n",
    "    spawning_importance = [1]\n",
    "\n",
    "if n_spawning == 2:\n",
    "    first_spawning = f\"{input('Define when the first spawning occurs (e.g. May). If it occurs in more than one month write April-May for example.')}\"\n",
    "    second_spawning = f\"{input('Define when the second spawning occurs:')}\"\n",
    "\n",
    "    # Importance of spawnings if they are more tha 1 spwning on a year:\n",
    "    spawning_importance = [float(input('What is the importance of the first spawning? If equal importance type 0.5')), float(input('What is the importance of the second spawning?'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC SETTINGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up he working directory:\n",
    "os.chdir(wd)\n",
    "\n",
    "# Extract netCDF file names and store the netCDFs:\n",
    "netcdfs = [nc for nc in os.listdir(wd) if \".nc\" in nc[-3:]]\n",
    "\n",
    "# Create a dictionary of months:\n",
    "months_dict = { \n",
    "    'January': 1, 'February': 2, 'March': 3, \n",
    "    'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, \n",
    "    'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# Create a Dataframe to store monthly means in each quarter:\n",
    "monthly_captures = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION SETTING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DECIMAL YEAR TO DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_year_to_ym(decimal_year):\n",
    "    year = np.floor(decimal_year).astype(int) \n",
    "    remainder = decimal_year - year  \n",
    "    month = np.floor(remainder * 12).astype(int) + 1 \n",
    "    return pd.to_datetime(year.astype(str) + '-' + month.astype(str).str.zfill(2), format='%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ACCUMULATE CATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_catches(dataframe, window):\n",
    "    dataframe['time'] = pd.to_datetime(dataframe['time']) # Extract timesteps of the dataframe\n",
    "    dataframe = dataframe.sort_values(by='time').reset_index(drop=True) \n",
    "    dataframe.set_index('time', inplace=True)\n",
    "    for i in range(1, window + 1):\n",
    "        col_name = f'RC_{i}'  # Name of the new column\n",
    "        dataframe[col_name] = dataframe.index.map(\n",
    "            lambda date: dataframe.loc[date - pd.DateOffset(years=i): date - pd.DateOffset(days=1), 'RC'].sum() \n",
    "            if (date - pd.DateOffset(years=i) in dataframe.index) \n",
    "            else np.nan\n",
    "        )\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ONE SPAWNING PERIOD: SPAWNING BIOMASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_previous_spawning_biomass(df, n, first_spawning):       \n",
    "    df['time'] = pd.to_datetime(df['time']) # Ensure 'time' column is datetime\n",
    "    df = df.sort_values(by='time').reset_index(drop=True) # Order the dataframe by 'time'    \n",
    "    df.set_index('time', inplace=True) # Create a temporal index based on the 'time' to facilitate searching values\n",
    "\n",
    "        # Process the months passed by the user:\n",
    "    first_spawning_months = first_spawning.replace(' ', '')  # Delete spaces\n",
    "    first_spw_months = first_spawning_months.split(',')  # Separate months\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        col_name = f\"RSB_{i}\"  # Name of the new column. RSB stands for Relative Spawning Biomass + _ + {number of years to take into account}\n",
    "        \n",
    "        # Map the dates to obtain the values during the previous years:\n",
    "        df[col_name] = df.index.map(\n",
    "            lambda date: (\n",
    "                df.loc[\n",
    "                    (df.index.year >= (date.year - i)) & (df.index.year <= (date.year - 1)) &\n",
    "                    (df.index.month.isin([months_dict[months] for months in first_spw_months]))  # Filter the months passed by the user\n",
    "                , 'relative_B'].mean() * i  # Compute efective RSB averaging the relative biomass when spawning happens (.mean()) - Accumulate the effective RSB during n years (i)\n",
    "            ) if (date.year - i) in df.index.year else np.nan  # If there is no data in n previous years return NaN\n",
    "        )\n",
    "\n",
    "        # Reset the index to put 'time' column in the original position\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df # Return dataframe with the new column with RSB_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TWO SPAWNING PERIODS: SPAWNING BIOMASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_previous_spawning_biomass(df, n, first_spawning, second_spawning, spawning_importance):\n",
    "    df['time'] = pd.to_datetime(df['time']) # Ensure 'time' column is datetime\n",
    "    df = df.sort_values(by='time').reset_index(drop=True) # Order the dataframe by 'time'   \n",
    "    df.set_index('time', inplace=True) # Create a temporal index based on the 'time' to facilitate searching values\n",
    "\n",
    "        # Process the months passed by the user:\n",
    "    first_spawning_months = first_spawning.replace(' ', '')  # Delete spaces\n",
    "    first_spw_months = first_spawning_months.split(',')  # Separate months\n",
    "\n",
    "    second_spawning_months = second_spawning.replace(' ', '') # Delete spaces\n",
    "    second_spw_months = second_spawning_months.split(',') # Separate months\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        col_name = f\"RSB_{i}\"  # Name of the new column. RSB stands for Relative Spawning Biomass + _ + {number of years to take into account}\n",
    "\n",
    "        # Map both spawning periods to obtain the wheighted means and accumulate them during n years\n",
    "        df[col_name] = df.index.map(\n",
    "            lambda date: (\n",
    "                ((\n",
    "                    # Compute the wheighted sum of the first period:\n",
    "                    (df.loc[\n",
    "                        (df.index.year >= (date.year - i)) & (df.index.year <= (date.year - 1)) & \n",
    "                        (df.index.month.isin([months_dict[months] for months in first_spw_months])), 'relative_B'\n",
    "                    ].sum() / len(first_spw_months)) * spawning_importance[0]\n",
    "                ) + ( \n",
    "                    # Compute the wheighted sum of the second period:\n",
    "                    (df.loc[\n",
    "                        (df.index.year >= (date.year - i)) & (df.index.year <= (date.year - 1)) & \n",
    "                        (df.index.month.isin([months_dict[months] for months in second_spw_months])), 'relative_B'\n",
    "                    ].sum() / len(second_spw_months)) * spawning_importance[1]\n",
    "                ))\n",
    "            ) if (date.year - i) in df.index.year else np.nan  # If there is no data in n previous years return NaN\n",
    "        )\n",
    "\n",
    "        # Reset the index to put 'time' column in the original position\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df # Return dataframe with the new column with RSB_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXECUTE TRAINING DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LANDING/CATCH RELATED PREDICTOR COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures_data = pd.read_csv(spict_data) # Read SPiCT input csv to avoid previous modifications\n",
    "\n",
    "captures_data['timeC'] = captures_data['timeC'].astype(float)   # Ensure 'timeC' is a floating type data\n",
    "    \n",
    "captures_data = captures_data.dropna(subset=['timeC'])  # Drop NoData\n",
    "\n",
    "# Loop in each quarter and compute the mean:\n",
    "for index, row in captures_data.iterrows():\n",
    "    year = int(row['timeC'])  \n",
    "    quarter = int((row['timeC'] - year) * 100 / 25) + 1  \n",
    "    \n",
    "    if quarter == 1:\n",
    "        months = [f\"{year}-01\", f\"{year}-02\", f\"{year}-03\"]\n",
    "    elif quarter == 2:\n",
    "        months = [f\"{year}-04\", f\"{year}-05\", f\"{year}-06\"]\n",
    "    elif quarter == 3:\n",
    "        months = [f\"{year}-07\", f\"{year}-08\", f\"{year}-09\"]\n",
    "    elif quarter == 4:\n",
    "        months = [f\"{year}-10\", f\"{year}-11\", f\"{year}-12\"]\n",
    "    \n",
    "    # Asign the mean of the captures to the corresponding months:\n",
    "    monthly_captures.extend([(month, row['obsC'] / 3) for month in months])\n",
    "\n",
    "# Convert the list to a Dataframe:\n",
    "monthly_captures_df = pd.DataFrame(monthly_captures, columns=['time', 'obsC'])\n",
    "\n",
    "# Convert to 'time' datatype:\n",
    "monthly_captures_df['time'] = pd.to_datetime(monthly_captures_df['time'])\n",
    "\n",
    "# Compute catch relative to Biomass at Maximum Sustainable Yield (catch/Bmsy):\n",
    "monthly_captures_df['RC'] = monthly_captures_df['obsC']/bmsy\n",
    "\n",
    "# Generate accumulation columns:\n",
    "accumulated_catches = accumulate_catches(monthly_captures_df, age_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPAWNING BIOMASS RELATED PREDICTOR COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawning_dataset = pd.read_csv(spict_data)  # Read SPiCT input csv to avoid previous modifications\n",
    "\n",
    "# Execute function to change 'time' column to a date type:\n",
    "spawning_dataset['time'] = decimal_year_to_ym(spawning_dataset['time'])\n",
    "\n",
    "# If two estimates are for one month compute the average to have just 1 relative biomass estimate by month:\n",
    "spawning_dataset_cleared = spawning_dataset.groupby('time', as_index=False)['relative_B'].mean()  # Notice that 'time' and 'relative_B' are the field names in the original SPiCT estimate csv\n",
    "\n",
    "# Run spawning biomass computation functions depending on the number of spawnings in a year:\n",
    "if n_spawning == 1:\n",
    "    spawning_predic = one_previous_spawning_biomass(spawning_dataset_cleared, age_max, first_spawning)   \n",
    "    \n",
    "if n_spawning == 2: \n",
    "    spawning_predic = two_previous_spawning_biomass(spawning_dataset_cleared, age_max, first_spawning, second_spawning, spawning_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENVIRONMENTAL PREDICTOR COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(spict_data)   # Read SPiCT input csv to avoid previous modifications\n",
    "\n",
    "# Execute function to change 'time' column to a date type:\n",
    "dataset['time'] = decimal_year_to_ym(dataset['time'])\n",
    "\n",
    "# If two estimates are for one month compute the average to have just 1 relative biomass estimate by month:\n",
    "dataset_cleared = dataset.groupby('time', as_index=False)['relative_B'].mean()  # Notice that 'time' and 'relative_B' are the field names in the original SPiCT estimate csv\n",
    "\n",
    "# Accumulator:\n",
    "i_bio = 0\n",
    "i_phy = 0\n",
    "\n",
    "# Loop the dataset and make the needed computations to prepare the input dataset:\n",
    "for env_netcdf in netcdfs:\n",
    "    # 1. Open the dataset and define the coordinate system:\n",
    "    try:\n",
    "        ds = xr.open_dataset(env_netcdf, engine=\"h5netcdf\")\n",
    "        ds = ds.rio.write_crs(env_epsg, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file with xarray: {e}\")\n",
    "\n",
    "    # 2. Mask the dataset to keep just the depth ranges that will be used:\n",
    "    depth_mask_up = ((ds['depth'] > 5) & (ds['depth'] < 10))\n",
    "    depth_mask_low = ((ds['depth'] > 45) & (ds['depth'] < 100))\n",
    "\n",
    "    # 3. Filter the datasets to keep just the desired depths:\n",
    "    ds_up = ds.where(depth_mask_up, drop=True)\n",
    "    ds_low = ds.where(depth_mask_low, drop=True)\n",
    "\n",
    "    # 4. Compute average values in the dimension 'depth':\n",
    "    mean_up = ds_up.mean(dim='depth')\n",
    "    mean_low = ds_low.mean(dim='depth')\n",
    "\n",
    "    # 5. Rename the variable names to be able to differentiate them:\n",
    "    if 'Biogeochemical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'chl': 'chl_up', 'nppv': 'npp_up','o2': 'o2_up','no3': 'no3_up','po4': 'po4_up','nh4': 'nh4_up','ph': 'ph_up'})\n",
    "        mean_low = mean_low.rename({'chl': 'chl_low', 'nppv': 'npp_low','o2': 'o2_low','no3': 'no3_low','po4': 'po4_low','nh4': 'nh4_low','ph': 'ph_low'})\n",
    "        \n",
    "    elif 'Physical' in env_netcdf:\n",
    "        mean_up = mean_up.rename({'so': 'so_up','thetao': 'thetao_up'})\n",
    "        mean_low = mean_low.rename({'so':'so_low', 'thetao': 'thetao_low'})\n",
    "        \n",
    "    else: \n",
    "        print(f\"{env_netcdf} netCDF file names are not included in conditions. {env_netcdf} will not be added to the training dataset! Check the code to adapt it!\")\n",
    "        continue  \n",
    "\n",
    "    # 6. Read the region of interest shapefile and check if the coordinate system of the environmental variables and the shapefile is the same:\n",
    "    roi = gpd.read_file(roi_file)\n",
    "    find_env_epsg = re.search(r'AUTHORITY\\[\"EPSG\",\"(\\d+)\"\\]', str(mean_low.rio.crs)) # Check coordinate system of env. data\n",
    "    if find_env_epsg.group(1) == roi.crs.to_epsg():\n",
    "        print(\"The EPSG codes of the environmental variables and the region of interest are the same!\")\n",
    "    else: \n",
    "        print(f\"The EPSG codes of the inputs are different! Environmental data projection is EPSG:{find_env_epsg.group(1)} and Region of Interest projection is EPSG:{roi.crs.to_epsg()}\")\n",
    "        print(f\"Converting Region of Interest file projection to EPSG:{find_env_epsg.group(1)}\")\n",
    "        roi = roi.to_crs(epsg=find_env_epsg.group(1))\n",
    "\n",
    "    # 7. Extract the geometry of the region of interest and clip the environmental data to the region of interest:\n",
    "    roi_mask = [mapping(roi.geometry[0])]   # Extract roi geometry\n",
    "    mean_up_clip = mean_up.rio.clip(roi_mask, roi.crs, drop=True)\n",
    "    mean_low_clip = mean_low.rio.clip(roi_mask, roi.crs, drop=True)\n",
    "\n",
    "    # 8. Compute the mean in each depth range and within the region of interest:\n",
    "    avg_up = mean_up_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    avg_low = mean_low_clip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # 9. Convert netCDF into a dataframe:\n",
    "    env_dataframe_up = avg_up.to_dataframe().reset_index()\n",
    "    env_dataframe_low = avg_low.to_dataframe().reset_index()\n",
    "\n",
    "    # 10. Filter the dataframes to drop the 'spatial_ref' column that can cause problems:\n",
    "    if 'spatial_ref' in env_dataframe_up.columns:\n",
    "        env_dataframe_up = env_dataframe_up.drop(columns=['spatial_ref'])\n",
    "    if 'spatial_ref' in env_dataframe_low.columns:\n",
    "        env_dataframe_low = env_dataframe_low.drop(columns=['spatial_ref'])\n",
    "\n",
    "    # 11. Merge the dataframes aligning the biomass estimates and environmental variable dates:\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_up, on='time', how='left')\n",
    "    dataset_cleared = pd.merge(dataset_cleared, env_dataframe_low, on='time', how='left')\n",
    "\n",
    "# 12. Create a list of the duplicate columns to merge:\n",
    "cols_to_combine = ['chl_up', 'npp_up', 'o2_up', 'no3_up', 'po4_up', 'nh4_up', 'ph_up',\n",
    "                   'chl_low', 'npp_low', 'o2_low', 'no3_low', 'po4_low', 'nh4_low', 'ph_low',\n",
    "                   'so_up', 'thetao_up', 'so_low', 'thetao_low']\n",
    "\n",
    "# 13. For each column, merge the versions \"_x\" & \"_y\" using combine_first():\n",
    "for col in cols_to_combine:\n",
    "    dataset_cleared[col] = dataset_cleared[f'{col}_x'].combine_first(dataset_cleared[f'{col}_y'])\n",
    "\n",
    "# 14. Delete the duplicate columns (_x, _y) as the data was already merged:\n",
    "dataset_cleared = dataset_cleared.drop(columns=[f'{col}_x' for col in cols_to_combine] + [f'{col}_y' for col in cols_to_combine])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MERGE AND CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATASET PREPARED!\n"
     ]
    }
   ],
   "source": [
    "# Merge 'dataset_cleared' with 'monthly_captures_df' to add captures into the training dataset:\n",
    "dataset_cleared = pd.merge(dataset_cleared, accumulated_catches, on='time', how='left')\n",
    "\n",
    "# Merge the new dataset to add the RSB predictors:\n",
    "dataset_cleared = pd.merge(dataset_cleared, spawning_predic, on=\"time\", how='left')\n",
    "\n",
    "# Create a folder to store the data:\n",
    "os.makedirs('Fisheries_modelling', exist_ok=True)\n",
    "\n",
    "# Save Raw dataset\n",
    "dataset_cleared.to_csv(os.path.join('Fisheries_modelling', 'raw_training_dataset.csv'))\n",
    "\n",
    "# Drop any timestep with Nan:\n",
    "dataset_cleared = dataset_cleared.dropna(how='any')\n",
    "\n",
    "# Clean: Copy relative Biomass in a column called RB and drop also 'obsC', 'RC', 'relative_B_x' and 'relative_B_y':\n",
    "dataset_cleared['RB'] = dataset_cleared['relative_B_x']\n",
    "dataset_cleared = dataset_cleared.drop(columns=[\"obsC\", \"RC\", \"relative_B_x\", \"relative_B_y\"])\n",
    "\n",
    "# Save the dataset that will be used to train the model:\n",
    "dataset_cleared.to_csv(os.path.join('Fisheries_modelling', 'training_dataset.csv'))\n",
    "\n",
    "print(\"TRAINING DATASET PREPARED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTOR SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunks we will execute the necessary methodologies to perform the selection of predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the csv with training data:\n",
    "training_data = pd.read_csv(os.path.join('Fisheries_modelling', 'training_dataset.csv'))\n",
    "\n",
    "# 2. Clear the csv if 'Unnamed' columns are in the training dataset:\n",
    "training_data = training_data.loc[:, ~training_data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# 3. Convert 'time' column into datetime type:\n",
    "training_data['time'] = pd.to_datetime(training_data['time'])\n",
    "\n",
    "# 4. Create a list with the first year of data and steps of 5 years (they will be used in the plots):\n",
    "start_year = training_data['time'].dt.year.min()\n",
    "end_year = training_data['time'].dt.year.max()\n",
    "years = np.arange(start_year, end_year + 1, 5)\n",
    "\n",
    "# 5. Predictor list:\n",
    "predictors_names = cols_to_combine + ['previous_relative_B']  # Adjust your predictors\n",
    "\n",
    "# 6. Create a loop to plot relative Biomass Index-Predictor Variable pairs:\n",
    "for predictor in predictors_names:\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # First axis for relative biomass:\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Relative Biomass', color='blue')\n",
    "    ax1.plot(training_data['time'], training_data['relative_B'], label='Relative Biomass', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')  # Axis colour\n",
    "\n",
    "    # Configure the ticks of the axis\n",
    "    ax1.set_xticks([pd.Timestamp(f'{year}-01-01') for year in years])  # Use the list of years\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Just show the year\n",
    "    \n",
    "    # Establish the limits for the x axis ticks:\n",
    "    ax1.set_xlim([training_data['time'].min() - pd.DateOffset(years=1), training_data['time'].max()])\n",
    "\n",
    "    \n",
    "    # Rotate the xaxis ticks\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Second axis for the predictor\n",
    "    ax2 = ax1.twinx()  # Create a second y axis that shares x axis\n",
    "    ax2.set_ylabel(predictor, color='green')\n",
    "    ax2.plot(training_data['time'], training_data[predictor], label=predictor, color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')  # y second axis colour\n",
    "    \n",
    "    # Plot title\n",
    "    plt.title(f'Relative Biomass and {predictor} over Time')\n",
    "\n",
    "    # Ensure axis disposition is correct\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Show plot:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisheries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
